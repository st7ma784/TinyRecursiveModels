Setup EMA
[Rank 0, World Size 1]: Epoch 0
TRAIN
W1017 21:41:57.460000 2591555 site-packages/torch/_inductor/utils.py:1250] [0/0] Not enough SMs to use max_autotune_gemm mode
/home/user/miniconda3/envs/open-ce/lib/python3.13/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: Tesla P100-PCIE-16GB does not support bfloat16 compilation natively, skipping
  warnings.warn(
/home/user/miniconda3/envs/open-ce/lib/python3.13/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: Tesla P100-PCIE-16GB does not support bfloat16 compilation natively, skipping
  warnings.warn(
/home/user/miniconda3/envs/open-ce/lib/python3.13/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: Tesla P100-PCIE-16GB does not support bfloat16 compilation natively, skipping
  warnings.warn(
/home/user/miniconda3/envs/open-ce/lib/python3.13/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: Tesla P100-PCIE-16GB does not support bfloat16 compilation natively, skipping
  warnings.warn(
/home/user/miniconda3/envs/open-ce/lib/python3.13/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: Tesla P100-PCIE-16GB does not support bfloat16 compilation natively, skipping
  warnings.warn(
/home/user/miniconda3/envs/open-ce/lib/python3.13/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: Tesla P100-PCIE-16GB does not support bfloat16 compilation natively, skipping
  warnings.warn(
/home/user/miniconda3/envs/open-ce/lib/python3.13/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: Tesla P100-PCIE-16GB does not support bfloat16 compilation natively, skipping
  warnings.warn(
/home/user/miniconda3/envs/open-ce/lib/python3.13/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: Tesla P100-PCIE-16GB does not support bfloat16 compilation natively, skipping
  warnings.warn(
/home/user/miniconda3/envs/open-ce/lib/python3.13/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: Tesla P100-PCIE-16GB does not support bfloat16 compilation natively, skipping
  warnings.warn(
/home/user/miniconda3/envs/open-ce/lib/python3.13/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: Tesla P100-PCIE-16GB does not support bfloat16 compilation natively, skipping
  warnings.warn(
/home/user/miniconda3/envs/open-ce/lib/python3.13/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: Tesla P100-PCIE-16GB does not support bfloat16 compilation natively, skipping
  warnings.warn(
/home/user/miniconda3/envs/open-ce/lib/python3.13/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: Tesla P100-PCIE-16GB does not support bfloat16 compilation natively, skipping
  warnings.warn(
Error executing job with overrides: ['arch=trm', 'data_paths=[data/lsa-9x9-10k]', 'evaluators=[{name: lsa@LSA}]', 'epochs=50000', 'eval_interval=5000', 'global_batch_size=128', 'lr=1e-4', 'puzzle_emb_lr=1e-4', 'weight_decay=1.0', 'puzzle_emb_weight_decay=1.0', 'arch.L_layers=2', 'arch.H_cycles=3', 'arch.L_cycles=6', '+run_name=pretrain_lsa_9x9_trm', 'ema=True']
Traceback (most recent call last):
  File "/data/TinyRecursiveModels/pretrain.py", line 633, in launch
    metrics = train_batch(config, train_state, batch, global_batch_size, rank=RANK, world_size=WORLD_SIZE)
  File "/data/TinyRecursiveModels/pretrain.py", line 329, in train_batch
    train_state.carry, loss, metrics, _, _ = train_state.model(carry=train_state.carry, batch=batch, return_keys=[])
                                             ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/miniconda3/envs/open-ce/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "/home/user/miniconda3/envs/open-ce/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/user/miniconda3/envs/open-ce/lib/python3.13/site-packages/torch/_dynamo/eval_frame.py", line 663, in _fn
    raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/miniconda3/envs/open-ce/lib/python3.13/site-packages/torch/_inductor/scheduler.py", line 3955, in create_backend
    raise GPUTooOldForTriton(device_props, inspect.currentframe())
torch._inductor.exc.GPUTooOldForTriton: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0

Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"


Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
