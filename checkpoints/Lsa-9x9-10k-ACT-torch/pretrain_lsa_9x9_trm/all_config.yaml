arch:
  H_cycles: 3
  H_layers: 0
  L_cycles: 6
  L_layers: 2
  expansion: 4
  forward_dtype: bfloat16
  halt_exploration_prob: 0.3
  halt_max_steps: 16
  hidden_size: 512
  hybrid_sinusoidal_ratio: 0.5
  loss:
    loss_type: stablemax_cross_entropy
    matrix_size: 9
    name: losses@ACTLossHead
    task_type: lsa
  mlp_t: false
  name: recursive_reasoning.trm@TinyRecursiveReasoningModel_ACTV1
  no_ACT_continue: true
  num_heads: 8
  pos_encodings: rope
  puzzle_emb_len: 16
  puzzle_emb_ndim: 512
  use_sinusoidal_value_embedding: true
  value_embedding_max: 100.0
  value_embedding_min: 0.0
  value_embedding_type: hybrid
beta1: 0.9
beta2: 0.95
checkpoint_every_eval: true
checkpoint_path: checkpoints/Lsa-9x9-10k-ACT-torch/pretrain_lsa_9x9_trm
data_paths:
- data/lsa-9x9-10k
data_paths_test: []
ema: true
ema_rate: 0.999
epochs: 500
eval_interval: 100
eval_save_outputs: []
evaluators:
- name: lsa@LSA
freeze_weights: false
global_batch_size: 128
load_checkpoint: null
lr: 0.0001
lr_min_ratio: 1.0
lr_warmup_steps: 2000
min_eval_interval: 0
project_name: Lsa-9x9-10k-ACT-torch
puzzle_emb_lr: 0.0001
puzzle_emb_weight_decay: 0.01
run_name: pretrain_lsa_9x9_trm
seed: 0
weight_decay: 0.01
